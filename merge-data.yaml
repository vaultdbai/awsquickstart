AWSTemplateFormatVersion: 2010-09-09
Description: VaultDB Lambda funtion to execute Queries
Metadata:
  Author: VaultDB.ai
  Url: https://www.vaultdb.ai

Parameters:
  ApplicationName:
    Type: String
    AllowedPattern: "^[a-z][a-z0-9-]{0,48}[a-z0-9]$"
    Description: Enter the name of your application with no spaces.

Resources:
  MergeDataFunction:
    Type: AWS::Lambda::Function
    Properties:
      Tags:
        - Key: Purpose
          Value: "VaultDB"
      Description: Merge Data into VaultDB Hub
      FunctionName: !Join ["", [!Ref ApplicationName, "-merge-data"]]
      Handler: index.lambda_handler
      Role:
        Fn::ImportValue: !Sub "${ApplicationName}-ExecuteRole"
      MemorySize: 128
      Timeout: 60
      Layers:
        - Fn::ImportValue: !Sub "${ApplicationName}-VaultDBPythonLayer"
      Runtime: python3.12
      Architectures:
        - x86_64
      Environment:
        Variables:
          application_name: !Sub "${ApplicationName}"
          AWS_STS_REGIONAL_ENDPOINTS: "regional"
          commitlog_directory: "/mnt/commitlog"
          public_bucket:
            Fn::ImportValue: !Sub "${ApplicationName}-PublicBucket"
          data_store:
            Fn::ImportValue: !Sub "${ApplicationName}-DataBucket"
          user_pool_id:
            Fn::ImportValue: !Sub "${ApplicationName}-UserPool"
          user_pool_client_id:
            Fn::ImportValue: !Sub "${ApplicationName}-UserPoolClient"
          identity_pool_id:
            Fn::ImportValue: !Sub "${ApplicationName}-IdentityPool"
      VpcConfig:
        SecurityGroupIds:
          - Fn::ImportValue: !Sub "${ApplicationName}-DataSecurityGroup"
        SubnetIds:
          - Fn::ImportValue: !Sub "${ApplicationName}-VPCPrivateSubnet"
      FileSystemConfigs:
        - Arn:
            Fn::ImportValue: !Sub "${ApplicationName}-EFSAccessPointResource"
          LocalMountPath: /mnt/commitlog
      Code:
        ZipFile: |
          # Imports
          import os
          import cfnresponse
          import logging
          import json
          import duckdb
          import boto3

          # Set up the logger
          logger = logging.getLogger()
          logger.setLevel(logging.DEBUG) # Very verbose

          application_name = os.environ['application_name'] if "application_name" in os.environ else ""
          commitlog_directory = os.environ['commitlog_directory'] if "commitlog_directory" in os.environ else "/tmp"
          public_bucket = os.environ['public_bucket'] if "public_bucket" in os.environ else None
          data_store = os.environ['data_store'] if "data_store" in os.environ else None
          app_client_id = os.environ['user_pool_client_id'] if "user_pool_client_id" in os.environ else None


          def lambda_handler(event, context):
              logger.info(f'event: {event}')

              try:
                  # Create a Boto3 S3 client
                  s3_client = boto3.client('s3')

                  for record in event['Records']:
                      source_bucket = record['s3']['bucket']['name']
                      logger.info(f'source_bucket: {source_bucket}')
                      file_key = record['s3']['object']['key']
                      logger.info(f'file_key: {file_key}')
                      preferred_role = file_key.split('/')[1]    
                      logger.info(f'preferred_role: {preferred_role}')
                      database_name = file_key.split('/')[-2]
                      logger.info(f'database_name: {database_name}')
                      db_path = f"{commitlog_directory}/{database_name}.db"

                      if not os.path.isfile(db_path):
                          return send_response(event, context, cfnresponse.FAILED, {'error':f"Catalog {database_name} does not exist!"})
                          
                      # Retrieve the object from S3
                      response = s3_client.get_object(Bucket=source_bucket, Key=file_key)            
                      response.read()
                      # Read the file content line by line
                      file_content = response['Body'].iter_lines()
                      if not file_content:
                          return send_response(event, context, cfnresponse.FAILED, {'error':"empty file contents!"})

                      # connect to database
                      connection = duckdb.connect(db_path, False, preferred_role, config={'autoinstall_known_extensions' : 'true'})
                      connection.execute(f"PRAGMA disable_data_inheritance;")
                      # Process each line of the file
                      counter = 0
                      for line in file_content:
                          stmt = line.decode('utf-8').strip()
                          if stmt:
                              logger.info(f'Executing Statement: {line}')
                              stmt_result = connection.execute(stmt)
                              counter+=1
                              logger.info(f'Statement Result: {stmt_result.fetchdf()}')

                      if counter:
                          connection.execute(f"PRAGMA enable_data_inheritance;")
                          stmt_result = connection.execute(f"MERGE DATABASE {database_name};")
                          logger.info(f'Statement Result: {stmt_result.fetchdf()}')

                      head, tail = os.path.split(file_key)
                      logger.info(f'head: {head}')
                      paginator = s3_client.get_paginator('list_objects_v2')
                      result = paginator.paginate(Bucket=source_bucket, Prefix=head)
                      for page in result:
                          if "Contents" in page:
                              for key in page[ "Contents" ]:
                                  copy_source = {
                                      'Bucket': source_bucket,
                                      'Key': key[ "Key" ]
                                      }
                                  logger.info(f'copy_source: {copy_source}')
                                  s3_client.copy(copy_source, source_bucket, f"archived/{key[ 'Key' ]}", ExtraArgs=None, Callback=None, SourceClient=None, Config=None)
                                  del_response = s3_client.delete_object(Bucket=source_bucket, Key=key[ "Key" ])
                                  if del_response["HTTPStatusCode"]!="204":
                                      logger.error(f'del_response: {del_response}')
                                      return send_response(event, context, cfnresponse.FAILED, {'error':f"couldn't archive file {key[ 'Key' ]}"})
                                  
                  return send_response(event, context, cfnresponse.SUCCESS, {'result':'success'})
              except Exception as ex:
                  logger.error(ex)
                  return send_response(event, context, cfnresponse.FAILED, {'error':str(ex)})


          def send_response(event, context, result, message):
              if "ResponseURL" in event:
                  cfnresponse.send(event, context, result, message)
              elif result==cfnresponse.FAILED:
                  return {"result":"Error", "message":message}
              else:
                  return {"result":"Success", "message":message}

  S3InvokeLambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !GetAtt MergeDataFunction.Arn
      Principal: s3.amazonaws.com
      SourceArn: 
        Fn::ImportValue: !Sub "${ApplicationName}-PublicBucket-arn"

  ScheduleMergedRule:
    Type: AWS::Events::Rule
    Properties:
      Description: "Scheduled data merge Runs"
      ScheduleExpression: "cron(0 17 * * ? *)" # Runs every day at 5 PM UTC
      State: "DISABLED"
      Targets:
        - Arn: !GetAtt MergeDataFunction.Arn
          Id: "UpdateKeysWeekly"
          Input: '{"RequestType":  "Refresh"}'

  PermissionForEventsToInvokeMergeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref MergeDataFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: !GetAtt MergeDataFunction.Arn

  InvokeDeploymentToConfigureTriggersAndUsers:
    Type: 'Custom::LambdaTrigger'
    DependsOn: 
      - S3InvokeLambdaPermission
      - ScheduleMergedRule
      - PermissionForEventsToInvokeMergeLambda
    Properties:
      ServiceToken: 
        Fn::ImportValue: !Sub "${ApplicationName}-DeploymentUtilFunction-arn"
      LambdaArn: !GetAtt MergeDataFunction.Arn

Outputs:
  MergeDataFunction:
    Description: VaultDB Data Merge Function
    Value: !Ref MergeDataFunction
    Export:
      Name: !Sub ${ApplicationName}-MergeDataFunction
  CLI:
    Description: Use this command to copy files
    Value: !Sub |
      aws lambda invoke --function-name '${ApplicationName}-merge-data' --payload '{ "Bucket": "${ApplicationName}-PublicBucket", "path": "merge_queue" }' lambda-output.txt --cli-binary-format raw-in-base64-out
