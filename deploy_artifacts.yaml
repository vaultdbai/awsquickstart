AWSTemplateFormatVersion: 2010-09-09
Description: VaultDB Lambda funtion to execute Queries
Metadata:
  Author: VaultDB.ai
  Url: https://vaultdb.ai

Parameters:
  ApplicationName:
    Type: String
    AllowedPattern: "^[a-z][a-z0-9-]{0,48}[a-z0-9]$"
    Description: Enter the name of your application with no spaces.

  SourceBucketName:
    Type: String
    Description: Vaultdb Artifact store.

  AdminEmail:
    Description: VaultDB Admin Email for default database user notifications
    Type: String
    AllowedPattern: '[^@]+@[^@]+\.[^@]+'
    Default: vaultdb@outlook.com

Resources:
  ExecuteUtilRole:
    Type: AWS::IAM::Role
    Properties:
      RoleName: !Join ["", ["vaultdb", "-execute-util-", !Ref ApplicationName]]
      Tags:
        - Key: Purpose
          Value: "VaultDB"
      AssumeRolePolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - sts:AssumeRole
      Policies:
        - PolicyName:
            !Join [
              "",
              ["vaultdb", "-execute-util-policy-", !Ref ApplicationName],
            ]
          PolicyDocument:
            Version: "2012-10-17"
            Statement:
              - Effect: Allow
                Action:
                  - logs:CreateLogGroup
                Resource: "*"
              - Effect: Allow
                Action:
                  - logs:CreateLogStream
                  - logs:PutLogEvents
                Resource: "*"
              - Effect: Allow
                Action:
                  - s3:ListMultipartUploadParts
                  - s3:*Object
                  - s3:ListBucket
                Resource: "*"

  DeploymentUtilFunction:
    Type: AWS::Lambda::Function
    Properties:
      Tags:
        - Key: Purpose
          Value: "VaultDB"
      Description: Execute Vaultdb SQL Qeries
      FunctionName:
        !Join ["", ["vaultdb", "-deployment-util-", !Ref ApplicationName]]
      Handler: index.lambda_handler
      MemorySize: 128
      Runtime: python3.8
      Role: !GetAtt ExecuteUtilRole.Arn
      Timeout: 60
      Environment:
        Variables:
          application_name: !Sub "${ApplicationName}"
          admin_email: !Sub "${AdminEmail}"
          AWS_STS_REGIONAL_ENDPOINTS: "regional"
          aws_region: !Sub "${AWS::Region}"
          user_pool_id:
            Fn::ImportValue: !Sub "${ApplicationName}-UserPool"
          user_pool_client_id:
            Fn::ImportValue: !Sub "${ApplicationName}-UserPoolClient"
          identity_pool_id:
            Fn::ImportValue: !Sub "${ApplicationName}-IdentityPool"
          source_bucket: !Ref SourceBucketName
          public_bucket:
            Fn::ImportValue: !Sub "${ApplicationName}-PublicBucket"
          data_bucket:
            Fn::ImportValue: !Sub "${ApplicationName}-DataBucket"
      Code:
        ZipFile: |
          import boto3
          import cfnresponse
          import botocore
          import os
          import logging          

          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          s3 = boto3.resource('s3')
          aws_region = os.environ['aws_region']
          user_pool_id = os.environ['user_pool_id']
          source_bucket = os.environ['source_bucket']
          public_bucket = os.environ['public_bucket']
          data_bucket = os.environ['data_bucket']

          def lambda_handler(event, context):
              try:
                  logger.info(f"event: {event}!")               
                  if event['RequestType'] == 'Delete':
                      for obj in s3.Bucket(public_bucket).objects.filter():
                          s3.Object(public_bucket, obj.key).delete()
                      try:
                          for obj in s3.Bucket(data_bucket).objects.filter():
                              s3.Object(data_bucket, obj.key).delete()
                      except Exception as e2:
                          print(e2)
                  elif event['RequestType'] == 'Refresh':
                      create_public_keys(data_bucket)
                  else:
                      logger.info("New files uploaded to the source bucket.")
                      deploy_folder(source_bucket, public_bucket, folder_name='workbench/')
                      deploy_folder(source_bucket, public_bucket, folder_name='release/')
                      create_pool_cofig(public_bucket)            
                      # create_welcome_page(public_bucket)
                      create_public_keys(data_bucket)
                  cfnresponse.send(event, context, cfnresponse.SUCCESS, {'result':'success'}, "CustomResourcePhysicalID")
              except Exception as e:
                  print(e)
                  cfnresponse.send(event, context, cfnresponse.FAILED, {'error':str(e)}, "CustomResourcePhysicalID")

          def create_public_keys(data_bucket):
              import urllib.request
              logger.info("Creating JWT Public Keys Json!")
              keys_url = 'https://cognito-idp.{}.amazonaws.com/{}/.well-known/jwks.json'.format(aws_region, user_pool_id)
              with urllib.request.urlopen(keys_url) as f:
                  response = f.read()                  
              keysobject = s3.Object(bucket_name=data_bucket, key='jwks.json')
              keysobject.put(Body=response.decode('utf-8'))            
              logger.info("Created JWT Public Keys!")                  

          def create_welcome_page(public_bucket):
              logger.info("Creating Welcome page!")                  
              application_name = os.environ['application_name']
              admin_email = os.environ['admin_email']
              response = s3.meta.client.get_object(Bucket=source_bucket, Key="welcome_template.html")
              welcomehtml = response['Body'].read().decode('utf-8')
              try:
                  import pystache
                  data = {
                  "name": application_name,
                  "email": admin_email
                  }
                  welcomehtml = pystache.render(welcomehtml, data)
              except:
                  logger.info("template error")    
                  
              welcomeobject = s3.Object(bucket_name=public_bucket, key='index.html')
              welcomeobject.put(Body=welcomehtml)            
              logger.info("Created Welcome page!")                  

          def create_pool_cofig(public_bucket):
              application_name = os.environ['application_name']              
              user_pool_client_id = os.environ['user_pool_client_id']
              identity_pool_id = os.environ['identity_pool_id']

              logger.info(f"Creating workbench config file for pool {user_pool_id}!") 

              data_string = f"""window.APPLICATION_NAME = "{application_name}";
              window.REGION= "{aws_region}";
              window.USER_POOL_ID= "{user_pool_id}";
              window.USER_POOL_APP_CLIENT_ID= "{user_pool_client_id}";
              window.USER_IDENTITY_POOL_ID= "{identity_pool_id}";
              """

              object = s3.Object(bucket_name=public_bucket, key='workbench/config.js')
              object.put(Body=data_string)            
              logger.info("Created workbench config file!")                  

          def deploy_folder(source_bucket, public_bucket, folder_name, destination=None):
              try:                  
                  for key in s3.meta.client.list_objects(Bucket=source_bucket, Prefix=folder_name)['Contents']:
                      file = key['Key']
                      if file==folder_name:
                          continue
                      logger.info(f"Copying file {file}")
                      source = {'Bucket': source_bucket, 'Key': file}
                      if destination is not None:
                          file=file.replace(folder_name, destination)
                      response = s3.meta.client.copy(source, public_bucket, file)
                  logger.info(f"Copied {folder_name} files!")
              except botocore.exceptions.ClientError as error:
                  logger.error("There was an error copying the file to the destination bucket")
                  print('Error Message: {}'.format(error))        
              except botocore.exceptions.ParamValidationError as error:
                  logger.error("Missing required parameters while calling the API.")
                  print('Error Message: {}'.format(error))

  ScheduledRule:
    Type: AWS::Events::Rule
    Properties:
      Description: "Scheduled Key Refresh"
      ScheduleExpression: "cron(15 10 ? * 6L *)"
      State: "ENABLED"
      Targets:
        - Arn: !GetAtt DeploymentUtilFunction.Arn
          Id: "UpdateKeysWeekly"
          Input: '{"RequestType":  "Refresh"}'

  PermissionForEventsToInvokeLambda:
    Type: AWS::Lambda::Permission
    Properties:
      FunctionName: !Ref DeploymentUtilFunction
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: !GetAtt DeploymentUtilFunction.Arn

  DeployInvoke:
    Type: AWS::CloudFormation::CustomResource
    DependsOn: DeploymentUtilFunction
    Properties:
      ServiceToken: !GetAtt DeploymentUtilFunction.Arn

Outputs:
  CLI:
    Description: Use this command to copy files
    Value: !Sub |
      aws lambda invoke --function-name 'vaultdb--deployment-util-${ApplicationName}' --payload '{ "Bucket": "test_bucket", "path": "tes" }' lambda-output.txt --cli-binary-format raw-in-base64-out
